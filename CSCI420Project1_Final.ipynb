{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gSG0HKabcQGc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert CSV files into dataframes to extract methods from\n",
        "methods1 = pd.read_csv('/content/extracted_methods__ether_pad.csv', encoding='latin-1')\n",
        "methods2 = pd.read_csv('/content/extracted_methods__haraldk_twelvemonkeys.csv', encoding='latin-1')\n",
        "methods3 = pd.read_csv('/content/extracted_methods__sirthias_parboiled.csv', encoding='latin-1')\n",
        "methods4 = pd.read_csv('/content/extracted_methods__tinkerpop_blueprints.csv', encoding='latin-1')\n",
        "methods5 = pd.read_csv('/content/extracted_methods__hierynomus_sshj.csv', encoding='latin-1')\n",
        "\n",
        "# Combine dataframes into one\n",
        "methods = pd.concat([methods1, methods2], ignore_index=True)\n",
        "methods = pd.concat([methods, methods3], ignore_index=True)\n",
        "methods = pd.concat([methods, methods4], ignore_index=True)\n",
        "methods = pd.concat([methods, methods5], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LKPw0fxYpdGY"
      },
      "outputs": [],
      "source": [
        "# Code taken from Pre-processing lab notebook - refines data into usable methods\n",
        "def remove_duplicates(data):\n",
        "    \"\"\"Remove duplicate methods based on method content.\n",
        "    Almost Type-1 with the exception of comments\n",
        "    \"\"\"\n",
        "    return data.drop_duplicates(subset=\"Method Java\", keep=\"first\")\n",
        "\n",
        "def filter_ascii_methods(data):\n",
        "    \"\"\"Filter methods to include only those with ASCII characters.\"\"\"\n",
        "    data = data[data[\"Method Java\"].apply(lambda x: all(ord(char) < 128 for char in x))]\n",
        "    return data\n",
        "\n",
        "def remove_outliers(data, lower_percentile=5, upper_percentile=95):\n",
        "    \"\"\"Remove outliers based on method length.\"\"\"\n",
        "    method_lengths = data[\"Method Java\"].apply(len)\n",
        "    lower_bound = method_lengths.quantile(lower_percentile / 100)\n",
        "    upper_bound = method_lengths.quantile(upper_percentile / 100)\n",
        "    return data[(method_lengths >= lower_bound) & (method_lengths <= upper_bound)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xvQRtdpRvXQP"
      },
      "outputs": [],
      "source": [
        "from pygments.lexers.jvm import JavaLexer\n",
        "from pygments.lexers import get_lexer_by_name\n",
        "from pygments.token import Token\n",
        "import re\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FeFA9obXu_al"
      },
      "outputs": [],
      "source": [
        "# Code taken from Pre-processing lab notebook - refines data into usable methods\n",
        "def remove_boilerplate_methods(data):\n",
        "    \"\"\"Remove boilerplate methods like setters and getters.\"\"\"\n",
        "    boilerplate_patterns = [\n",
        "        r\"\\bset[A-Z][a-zA-Z0-9_]*\\(.*\\)\\s*{\",  # Setter methods\n",
        "        r\"\\bget[A-Z][a-zA-Z0-9_]*\\(.*\\)\\s*{\",  # Getter methods\n",
        "    ]\n",
        "    boilerplate_regex = re.compile(\"|\".join(boilerplate_patterns))\n",
        "    data = data[~data[\"Method Java\"].apply(lambda x: bool(boilerplate_regex.search(x)))]\n",
        "    return data\n",
        "\n",
        "def remove_comments_from_dataframe(df: pd.DataFrame, method_column: str, language: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes comments from Java methods in a DataFrame and adds a new column with cleaned methods.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing the methods.\n",
        "        method_column (str): Column name containing the raw Java methods.\n",
        "        language (str): Programming language for the lexer (e.g., 'java').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Updated DataFrame with a new column 'Java Method No Comments'.\n",
        "    \"\"\"\n",
        "    # Define a function to remove comments from a single method\n",
        "    def remove_comments(code):\n",
        "        lexer = get_lexer_by_name(language)\n",
        "        tokens = lexer.get_tokens(code)\n",
        "        # Filter out comments using a lambda function\n",
        "        clean_code = ''.join(token[1] for token in tokens if not (lambda t: t[0] in Token.Comment)(token))\n",
        "        return clean_code\n",
        "\n",
        "    # Apply the function to the specified column and add a new column with the results\n",
        "    df[\"Method Java No Comments\"] = df[method_column].apply(remove_comments)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-PgDiWuvx4u",
        "outputId": "3033b5af-c10b-490f-b124-8686af36673c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial dataset size: 228174\n",
            "After removing duplicates: 42516\n",
            "After filtering ASCII methods: 42432\n",
            "After removing outliers: 38339\n",
            "After removing boilerplate methods: 29373\n",
            "After cleaning comments: 29373\n"
          ]
        }
      ],
      "source": [
        "# Code taken from Pre-processing lab notebook - refines data into usable methods\n",
        "data = methods\n",
        "data = data.drop(\"Commit Hash\", axis=1)\n",
        "data = data.drop(\"File Name\", axis=1)\n",
        "data = data.drop(\"Method Name\", axis=1)\n",
        "data = data.drop(\"Commit Link\", axis=1)\n",
        "\n",
        "print(\"Initial dataset size:\", len(data))\n",
        "data = remove_duplicates(data)\n",
        "print(\"After removing duplicates:\", len(data))\n",
        "\n",
        "data = filter_ascii_methods(data)\n",
        "print(\"After filtering ASCII methods:\", len(data))\n",
        "\n",
        "data = remove_outliers(data)\n",
        "print(\"After removing outliers:\", len(data))\n",
        "\n",
        "data = remove_boilerplate_methods(data)\n",
        "print(\"After removing boilerplate methods:\", len(data))\n",
        "\n",
        "data = remove_comments_from_dataframe(data, \"Method Java\", \"Java\")\n",
        "print(\"After cleaning comments:\", len(data))\n",
        "\n",
        "# Create training/eval/test sets for student dataset\n",
        "student_methods = list(data[\"Method Java No Comments\"])\n",
        "# Randomize method order to avoid repository bias in sets\n",
        "random.Random(42).shuffle(student_methods)\n",
        "# These slice values correspond to an 80-10-10 set split\n",
        "training_student_methods = student_methods[:int(len(student_methods) * 0.8)]\n",
        "eval_student_methods = student_methods[int(len(student_methods) * 0.8 + 1):int(len(student_methods) * 0.9)]\n",
        "test_student_methods = student_methods[int(len(student_methods) * 0.9):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T-ytrdb3v4E1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Tokenizes all methods within a given dataframe\n",
        "\n",
        "Args:\n",
        "data (dataframe): Dataframe containing methods to tokenize\n",
        "\n",
        "Returns: A list of extracted tokens\n",
        "\"\"\"\n",
        "def tokenize(data):\n",
        "    lexer = JavaLexer(encoding='latin-1')\n",
        "    lexer.add_filter('whitespace', spaces=' ')\n",
        "    datas = data[\"Method Java No Comments\"].to_list()\n",
        "    datas = ' '.join(datas)\n",
        "    tokens = [t[1] for t in lexer.get_tokens(datas) if t[1] != ' ' and t[1] != '\\n' and t[1] != '\\t']\n",
        "    return tokens\n",
        "\n",
        "\"\"\"\n",
        "Tokenizes a given method\n",
        "\n",
        "Args:\n",
        "method (str): Method to tokenize\n",
        "\n",
        "Returns: A list of extracted tokens\n",
        "\"\"\"\n",
        "def tokenize_method(method):\n",
        "    lexer = JavaLexer(encoding='latin-1')\n",
        "    lexer.add_filter('whitespace', spaces=' ')\n",
        "    tokens = [t[1] for t in lexer.get_tokens(method) if t[1] != ' ' and t[1] != '\\n' and t[1] != '\\t']\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ioPHT4ZhQvkr"
      },
      "outputs": [],
      "source": [
        "# nltk and collections modules are used for creating ngrams\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8wrGY41U0ZQ8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creates ngrams for a given set of tokens\n",
        "\n",
        "Args:\n",
        "tokens (list of str): Tokenized data\n",
        "n (int): Window size\n",
        "\n",
        "Returns: Model containing n-grams and probabilities of next token\n",
        "\"\"\"\n",
        "# CODE DERIVED FROM https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk\n",
        "def create_ngrams(tokens, n):\n",
        "  n_gram = ngrams(tokens, n)\n",
        "  # Model will contain all ngrams along with the\n",
        "  # potentially predictable tokens and their probabilities\n",
        "  model_n = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "  # General form to increment the count of a given predictable token\n",
        "  for gram in n_gram:\n",
        "      model_n[gram[:-1]][gram[-1]] += 1\n",
        "\n",
        "  # Transform the counts into probabilities\n",
        "  for gram in model_n:\n",
        "      total_count = float(sum(model_n[gram].values()))\n",
        "      for pred_token in model_n[gram]:\n",
        "          model_n[gram][pred_token] /= total_count\n",
        "\n",
        "  return model_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YMuKxpCyW2s9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Predicts the next word based on the previous two words using the trained 5-gram model.\n",
        "Args:\n",
        "model (dictionary): Model with all known n-grams and their probabilities\n",
        "gram (tuple): Tokens to use to predict next token\n",
        "\n",
        "Returns: Tuple containing predicted next token and its probability.\n",
        "\"\"\"\n",
        "# CODE DERIVED FROM https://www.geeksforgeeks.org/n-gram-language-modelling-with-nltk\n",
        "def predict_next_word(model, gram):\n",
        "    next_word = model[gram]\n",
        "    if next_word:\n",
        "        # Choose the most likely next word\n",
        "        predicted_word = max(next_word, key=next_word.get)\n",
        "        predicted_prob = next_word[predicted_word]\n",
        "        return (predicted_word, predicted_prob)\n",
        "    # If no next valid word can be predicted, return an UNK token\n",
        "    else:\n",
        "        return (\"<UNK>\", 1)\n",
        "\n",
        "\"\"\"\n",
        "Iteratively creates a method based on predictions from given tokens\n",
        "\n",
        "Args:\n",
        "gram (tuple): First n-1 tokens to predict method with n-gram\n",
        "tokens (list): Tokens extracted from methods\n",
        "model_train (ngrams): The ngrams of the given training data\n",
        "include_probability (boolean): Determines whether to include probability of predicted token\n",
        "\n",
        "Returns: List of tokens (str) in predicted method\n",
        "(or tuple containing tokens (str) of method and their probabilities (str))\n",
        "\"\"\"\n",
        "def iterative_prediction(gram, tokens, model_train, include_probability=True):\n",
        "    # Counts number of predictions in the method so far\n",
        "    token_count = 0\n",
        "    context_size = len(gram) + 1\n",
        "    # Converts the n-gram to an iterable form (for unpacking in method args)\n",
        "    symbols = [gram]\n",
        "    symbols_iterable = list(*symbols)\n",
        "    # Stores what will become the finished predicted method\n",
        "    predicted_method = []\n",
        "    next_word = predict_next_word(model_train, *symbols)\n",
        "    brackets = 0\n",
        "    # If an opening bracket is present in the original tokens, need to account\n",
        "    # for it if we want to attempt to balance delimiters\n",
        "    for symbol in gram:\n",
        "        if symbol == '{':\n",
        "            brackets += 1\n",
        "    while token_count < 100:\n",
        "        # Sanity check to prevent potentially endless loops\n",
        "        token_count += 1\n",
        "        # Attempt to count delimiters for balancing\n",
        "        if next_word[0] == '{':\n",
        "            brackets += 1\n",
        "        elif next_word[0] == '}':\n",
        "            brackets -= 1\n",
        "        # Check whether to include probability in returned value\n",
        "        if include_probability:\n",
        "            predicted_method += [next_word]\n",
        "        else:\n",
        "            predicted_method += [next_word[0]]\n",
        "        # Check if delimiters are balanced - if they are, stop iterating\n",
        "        if next_word[0] == '}' and brackets == 0:\n",
        "            break\n",
        "        # Accounts for abstract methods, e.g.\n",
        "        if next_word[0] == ';' and brackets == 0:\n",
        "            break\n",
        "        # Shift tokens over to include new predicted token for next iteration\n",
        "        for i in range(len(symbols_iterable) - 1):\n",
        "            symbols_iterable[i] = symbols_iterable[i + 1]\n",
        "        symbols_iterable[len(symbols_iterable) - 1] = next_word[0]\n",
        "        symbols = tuple(symbols_iterable)\n",
        "        next_word = predict_next_word(model_train, symbols)\n",
        "\n",
        "    return predicted_method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JaUGYs7IIGyX"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\"\"\"\n",
        "Calculates the perplexity of a given method prediction\n",
        "\n",
        "Args:\n",
        "prediction (list): The generated method from an iterative prediction\n",
        "\n",
        "Returns: The perplexity of a given method prediction (float)\n",
        "\"\"\"\n",
        "def perplexity(prediction):\n",
        "    # Check if perplexity can be calculated. If not, won't calculate\n",
        "    if len(prediction) == 0:\n",
        "        return \"Unknown\"\n",
        "    elif isinstance(prediction[0], tuple):\n",
        "        perplexity_score = 1\n",
        "        for t in prediction:\n",
        "            perplexity_score *= 1/t[1]\n",
        "        perplexity_score = math.pow(perplexity_score, 1/len(prediction))\n",
        "        return perplexity_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vQymnXjTOu87"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Calculates the n-gram model's perplexity for a given evaluation set\n",
        "\n",
        "Args:\n",
        "dataset (list): List of holdout evaluation/testing methods obtained from dataset\n",
        "tokens (list): List of tokens/vocabulary\n",
        "model_train (ngrams): N-grams of given data\n",
        "context_size (int): Size of n-gram context\n",
        "\"\"\"\n",
        "def evaluate_model_perplexity(dataset, tokens, model_train, context_size):\n",
        "    valid_methods = 0\n",
        "    perplexity_sum = 0\n",
        "    for method in dataset:\n",
        "        method = tokenize_method(method)\n",
        "        first_n_tokens = method[0:context_size-1]\n",
        "        #print(\"Original method:\\n\" + str(' '.join(method)))\n",
        "        #pred = ' '.join(iterative_prediction(tuple(first_n_tokens), teacher_tokens, model_train, include_probability=False))\n",
        "        #print(' '.join(first_n_tokens) + \" \" + pred)\n",
        "        pred_verbose = iterative_prediction(tuple(first_n_tokens), tokens, model_train, include_probability=True)\n",
        "        #print(pred_verbose)\n",
        "        method_perplexity = perplexity(pred_verbose)\n",
        "        if method_perplexity != \"Unknown\" and pred_verbose[0][0] != \"<UNK>\":\n",
        "            valid_methods += 1\n",
        "            perplexity_sum += method_perplexity\n",
        "        #print(\"Method Perplexity: \" + str(perplexity(pred_verbose)))\n",
        "        #print('\\n')\n",
        "    average_perplexity = perplexity_sum/valid_methods\n",
        "    print(f'AVERAGE PERPLEXITY OF MODEL WITH CONTEXT SIZE {context}: {average_perplexity}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin curating dataset for instructor-provided corpus\n",
        "method_file = 'training.txt'\n",
        "\n",
        "teacher_methods = list(open(method_file).read().splitlines())\n",
        "\n",
        "#Randomize methods for train/test/validation split\n",
        "random.Random(50).shuffle(teacher_methods)\n",
        "#80-10-10 split\n",
        "training_teacher_methods = teacher_methods[:80001]\n",
        "eval_teacher_methods = teacher_methods[80001:90001]\n",
        "test_teacher_methods = teacher_methods[90001:]"
      ],
      "metadata": {
        "id": "ZNvCB7VkmPAp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize teacher dataset\n",
        "teacher_tokens = []\n",
        "for method in training_teacher_methods:\n",
        "    teacher_tokens += tokenize_method(method)"
      ],
      "metadata": {
        "id": "AxTbZVqBmNKp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize student dataset\n",
        "student_tokens = []\n",
        "for method in training_student_methods:\n",
        "    student_tokens += tokenize_method(method)"
      ],
      "metadata": {
        "id": "2HIWvRdbvP62"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of student vocabulary\n",
        "print(len(set(student_tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf9cEpy7SrZ6",
        "outputId": "e92a6271-d837-475c-e696-2da2dadee0ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finds and displays average perplexity of n-gram models for n=3, n=5, n=7, n=9\n",
        "context = 3\n",
        "model_train = create_ngrams(student_tokens, context)\n",
        "evaluate_model_perplexity(eval_student_methods, student_tokens, model_train, context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXLtdEl_J9e6",
        "outputId": "03141e0f-5c72-45b5-b481-c9a4a000fd79"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVERAGE PERPLEXITY OF MODEL WITH CONTEXT SIZE 3: 3.4814875263359037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = 5\n",
        "model_train = create_ngrams(student_tokens, context)\n",
        "evaluate_model_perplexity(eval_student_methods, student_tokens, model_train, context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv_mTzbalx_x",
        "outputId": "01ac2df3-1d15-43c1-aa11-99e0081ef47c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVERAGE PERPLEXITY OF MODEL WITH CONTEXT SIZE 5: 1.6179664078909373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = 7\n",
        "model_train = create_ngrams(student_tokens, context)\n",
        "evaluate_model_perplexity(eval_student_methods, student_tokens, model_train, context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF8GKVa6lyO1",
        "outputId": "ae0d6271-69e3-4f9b-f32d-7d8fb3b733a4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVERAGE PERPLEXITY OF MODEL WITH CONTEXT SIZE 7: 1.2561000162692226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing best-performing student model on test set\n",
        "optimal_student_context = 7\n",
        "model_train = create_ngrams(student_tokens, optimal_student_context)\n",
        "evaluate_model_perplexity(test_student_methods, student_tokens, model_train, optimal_student_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZAjrY8sm2RS",
        "outputId": "dddf09a4-2628-460d-8eb8-65ba31003cd5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVERAGE PERPLEXITY OF MODEL WITH CONTEXT SIZE 7: 1.2497774207404262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WRITES OPTIMAL MODEL'S TEST METHOD OUTPUT TO A .TXT FILE; NOT NECESSARY TO RUN\n",
        "# If running, make sure the previous block has been run before it, or model_train will be incorrect\n",
        "\"\"\"\n",
        "i = 0\n",
        "dict_to_dump = dict()\n",
        "for method in test_student_methods[:100]:\n",
        "    method = tokenize_method(method)\n",
        "    dict_to_dump[str(i)] = str(iterative_prediction(tuple(method[0:optimal_student_context-1]), student_tokens, model_train, include_probability=True))\n",
        "    i += 1\n",
        "\n",
        "out_file = open(\"results_student_model.txt\", \"w\")\n",
        "for i in range(0, len(dict_to_dump)):\n",
        "    out_file.write('\"' + str(i) + '\": ' + str(dict_to_dump[str(i)]) + '\\n\\n')\n",
        "out_file.close()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7CJ152LzJXWB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finds and displays average perplexity of n-gram models for n=3, n=5, n=7, n=9\n",
        "# WARNING: This code block takes around 10 minutes to fully execute\n",
        "context = 3\n",
        "model_train = create_ngrams(teacher_tokens, context)\n",
        "evaluate_model_perplexity(eval_teacher_methods, teacher_tokens, model_train, context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRVR4tAVewru",
        "outputId": "1dae919a-fcb1-4658-cc10-1ee1a061eab0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVERAGE PERPLEXITY OF MODEL WITH CONTEXT SIZE 3: 1.762809392841246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = 5\n",
        "model_train = create_ngrams(teacher_tokens, context)\n",
        "evaluate_model_perplexity(eval_teacher_methods, teacher_tokens, model_train, context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAr1BG4JDkMp",
        "outputId": "7cc82ef2-f5db-40ea-98e7-30a0832e5f83"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVERAGE PERPLEXITY OF MODEL WITH CONTEXT SIZE 5: 1.965038245542253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = 7\n",
        "model_train = create_ngrams(teacher_tokens, context)\n",
        "evaluate_model_perplexity(eval_teacher_methods, teacher_tokens, model_train, context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRFcvW2qDlQZ",
        "outputId": "5065765a-279d-4d24-f526-7d4b9cfc7c41"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVERAGE PERPLEXITY OF MODEL WITH CONTEXT SIZE 7: 1.8768182841564058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kENFVqzA5L4n",
        "outputId": "89e382a9-e5ca-4819-fc85-202b1b199cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVERAGE PERPLEXITY OF MODEL WITH CONTEXT SIZE 7: 1.761250418620468\n"
          ]
        }
      ],
      "source": [
        "# Testing best-performing instructor-provided model on test set\n",
        "optimal_teacher_context = 3\n",
        "model_train = create_ngrams(teacher_tokens, optimal_teacher_context)\n",
        "evaluate_model_perplexity(test_student_methods, teacher_tokens, model_train, optimal_teacher_context)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WRITES OPTIMAL MODEL'S TEST METHOD OUTPUT TO A .TXT FILE; NOT NECESSARY TO RUN\n",
        "# If running, make sure the previous block has been run before it, or model_train will be incorrect\n",
        "\"\"\"\n",
        "i = 0\n",
        "dict_to_dump = dict()\n",
        "for method in test_student_methods[:100]:\n",
        "    method = tokenize_method(method)\n",
        "    dict_to_dump[str(i)] = str(iterative_prediction(tuple(method[0:optimal_teacher_context-1]), teacher_tokens, model_train, include_probability=True))\n",
        "    i += 1\n",
        "\n",
        "out_file = open(\"results_teacher_model.txt\", \"w\")\n",
        "for i in range(0, len(dict_to_dump)):\n",
        "    out_file.write('\"' + str(i) + '\": ' + str(dict_to_dump[str(i)]) + '\\n\\n')\n",
        "out_file.close()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bnTutBQF3pd3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y1k8aegu3IVu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}